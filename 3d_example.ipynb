{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a166e35f",
   "metadata": {},
   "source": [
    "This notebook investigates using the polytope sampler for the 3d example from Paper1/2.\n",
    "\n",
    "1. Polytope Sampler `../pilot_study_unfolding.ipynb`\n",
    "\n",
    "The example is:\n",
    "\\begin{equation}\n",
    "    \\boldsymbol y = \\boldsymbol x^* + \\varepsilon, \\; \\; \\varepsilon \\sim N(\\boldsymbol 0, \\boldsymbol I_3), \\; \\; \\boldsymbol x \\geq 0,\n",
    "\\end{equation}\n",
    "where $\\boldsymbol x^* = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}^T$, and our functional of interest is defined as $\\boldsymbol h = \\begin{pmatrix} 1 & 1 & -1 \\end{pmatrix}^T$.\n",
    "\n",
    "Alternatively, we also examine points of the form $\\boldsymbol x^* = \\begin{pmatrix} t & t & 1 \\end{pmatrix}^T$, where $t >0$.\n",
    "\n",
    "__This notebook does the following__\n",
    "1. Goes through one example of the polytope sampler applied to the 3d problem\n",
    "2. Generates the chains for the interval computation\n",
    "3. Analyzes the 3d output\n",
    "4. Investigates quantiles on the space $\\{x: h^Tx = h^T x^*, x \\geq 0 \\}$.\n",
    "5. Test out a new importance sampler that move heavily samples near the parameter boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaFuncCI.sample import ellipsoidSampler, polytopeSampler\n",
    "from adaFuncCI.llr import llrSolver_3d\n",
    "from adaFuncCI.inversion_intervals import solve_llr_fixed_y\n",
    "from adaFuncCI.inversion_intervals import direct_inversion\n",
    "from adaFuncCI.inversion_intervals import max_local_quantile_inversion\n",
    "from adaFuncCI.optimize import osb_int, ssb_int\n",
    "from adaFuncCI.max_quantile import maxQuantileRS\n",
    "from adaFuncCI.utils import int_cover, percentile_ci_idx\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Define the LaTeX preamble to include multiple packages\n",
    "plt.rcParams['text.latex.preamble'] = r'''\n",
    "\\usepackage{amsmath}\n",
    "\\usepackage{amssymb}\n",
    "\\usepackage{bm}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b8aaf",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023038b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fixed experiment settings\n",
    "# x_star = np.array([0., 0., 1.])\n",
    "t = 0.03\n",
    "x_star = np.array([t, t, 1.])\n",
    "h = np.array([1, 1, -1])\n",
    "noise_distr = stats.norm\n",
    "N = 1000  # number of data draws\n",
    "\n",
    "# uncertainty parameters\n",
    "alpha = 0.32\n",
    "eta = 0.01\n",
    "gamma = alpha - eta\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faadd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true functional\n",
    "np.dot(h, x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452253c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate noise\n",
    "np.random.seed(11211)\n",
    "noise = noise_distr.rvs(size=(N, 3))\n",
    "data = x_star + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de689b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate quantile at this point\n",
    "llr_3d = llrSolver_3d()\n",
    "np.percentile(llr_3d.solve_llr(x=x_star, E=stats.norm.rvs(size=30000).reshape((10000, 3))), q=68)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad75b34",
   "metadata": {},
   "source": [
    "# 1 - One example of using polytope sampler versus ellipsoid sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808f1a7",
   "metadata": {},
   "source": [
    "\"Bad\" example: 746\n",
    "\n",
    "\"Normal\" example: 0\n",
    "\n",
    "Numerically annoying example: 452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb857ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary with sampler properties\n",
    "mcmc_dict = {\n",
    "    'N_hp': 6,\n",
    "    'radius': 0.5,\n",
    "    'polytope_type': 'eigen',\n",
    "    'mcmc_alg': 'vaidya'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b37323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sampler(\n",
    "    data_i, M, x_center=np.zeros(3),\n",
    "    eta=eta, K=np.identity(3), h=h,\n",
    "    ap_gamma=0.5, ap_ord=0.25,\n",
    "    mcmc_hp_dict=mcmc_dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper around the Polytope algorithm that includes an extra\n",
    "    accept/reject step.\n",
    "    \n",
    "    The accept probability is defined by exp(-ap_gamma * norm(x - x_center)_{ap_ord}).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        data_i       (np arr) : data vector\n",
    "        M            (int)    : number of total samples to draw\n",
    "        x_center     (np arr) : center location of acceptance prob calc\n",
    "        eta          (float)  : BB set prob\n",
    "        K            (np arr) : forward model\n",
    "        h            (np arr) : functional vector\n",
    "        ap_gamma     (float)  : \"accept-probability\" gamma\n",
    "        ap_ord       (float)  : \"accept-probability\" order of norm (should <1)\n",
    "        mcmc_hp_dict (dict)   : hyperparameter for mcmc algo\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        final_sample (np arr) : complete sample\n",
    "        \n",
    "    \"\"\"\n",
    "    # declare sampler\n",
    "    sampler = polytopeSampler(\n",
    "        y=data_i,\n",
    "        eta=eta,\n",
    "        K=K,\n",
    "        h=h,\n",
    "        N_hp=mcmc_hp_dict['N_hp'],\n",
    "        r=mcmc_hp_dict['radius'],\n",
    "        random_seed=None,\n",
    "        polytope_type=mcmc_hp_dict['polytope_type'],\n",
    "        alg=mcmc_hp_dict['mcmc_alg'],\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "    \n",
    "    num_samples = 0\n",
    "    final_sample = np.zeros(shape=(M, K.shape[1]))\n",
    "    prev_idx = 0\n",
    "    curr_idx = 0\n",
    "    while num_samples < M:\n",
    "\n",
    "        # draw samples from polytope\n",
    "        param_draws = sampler.sample_mixed_ensemble(M=M)\n",
    "\n",
    "        # compute their accept reject probabilies\n",
    "        accept_probs = np.exp(\n",
    "            -ap_gamma * np.linalg.norm(param_draws - x_center, ord=ap_ord, axis=1)\n",
    "        )\n",
    "\n",
    "        # decide to accept/reject\n",
    "        accept_reject = stats.bernoulli(accept_probs).rvs()\n",
    "        \n",
    "        # save accepted points\n",
    "        curr_idx = prev_idx + accept_reject.sum()\n",
    "        \n",
    "        if curr_idx > M:\n",
    "            curr_idx = M\n",
    "        \n",
    "        final_sample[prev_idx:curr_idx, :] = param_draws[\n",
    "            accept_reject==1, :\n",
    "        ][:(curr_idx - prev_idx)]\n",
    "        \n",
    "        # updates\n",
    "        prev_idx = curr_idx\n",
    "        num_samples += accept_reject.sum()\n",
    "\n",
    "    return final_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1cfa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate sample from the importance sampler\n",
    "OBS_IDX = 0\n",
    "sample_is = importance_sampler(\n",
    "    data_i=data[OBS_IDX], M=16000, ap_gamma=0.75, ap_ord=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60040bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draws from the original sampler\n",
    "sampler = polytopeSampler(\n",
    "    y=data[OBS_IDX],\n",
    "    eta=eta,\n",
    "    K=np.identity(3),\n",
    "    h=h,\n",
    "    N_hp=6,\n",
    "    r=0.5,\n",
    "    random_seed=None,\n",
    "    polytope_type='eigen',\n",
    "    alg='vaidya',\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "# draw parameter sample in the BB set\n",
    "param_draws = sampler.sample_mixed_ensemble(M=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create curve in x1/x2 plane\n",
    "x_grid = np.linspace(0, 4, num=200)\n",
    "x2_vals = data[OBS_IDX][1] + np.sqrt(stats.chi2(3).ppf(1 - eta) - (data[OBS_IDX][0] - x_grid) ** 2 - data[OBS_IDX][2] ** 2)\n",
    "x2_nz = x2_vals >= 0\n",
    "\n",
    "# create the curve in x1/x3 plane\n",
    "x3_vals = data[OBS_IDX][2] + np.sqrt(stats.chi2(3).ppf(1 - eta) - (data[OBS_IDX][0] - x_grid) ** 2 - data[OBS_IDX][1] ** 2)\n",
    "x3_nz = x3_vals >= 0\n",
    "\n",
    "# create curve in the x2/x3 plane\n",
    "x23_vals = data[OBS_IDX][2] + np.sqrt(stats.chi2(3).ppf(1 - eta) - data[OBS_IDX][0] ** 2 - (data[OBS_IDX][1] - x_grid) ** 2)\n",
    "x23_nz = x23_vals >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(12, 4), sharex=True, sharey=True)\n",
    "\n",
    "# 1 + 2\n",
    "ax[0].scatter(sample_is[:, 0], sample_is[:, 1], alpha=0.15, s=5)\n",
    "ax[0].scatter([x_star[0]], [x_star[1]], color='red')\n",
    "ax[0].plot(x_grid[x2_nz], x2_vals[x2_nz], color='orange', label='Berger--Boos Set')\n",
    "ax[0].axhline(2.38890183e+00, color='orange', xmin=0, xmax=(3.88756344e+00)/4, linestyle=':', label='Polytope Boundary')\n",
    "ax[0].axvline(3.88756344e+00, color='orange', ymin=0, ymax=(2.38890183e+00)/4, linestyle=':')\n",
    "\n",
    "# 1 + 3\n",
    "ax[1].scatter(sample_is[:, 0], sample_is[:, 2], alpha=0.15, s=5)\n",
    "ax[1].scatter([x_star[0]], [x_star[2]], color='red')\n",
    "ax[1].plot(x_grid, x3_vals, color='orange')\n",
    "ax[1].axhline(4.05231276e+00, color='orange', xmin=0, xmax=0.96, linestyle=':', label='Polytope Boundary')\n",
    "ax[1].axvline(3.88756344e+00, color='orange', ymin=0, ymax=0.96, linestyle=':')\n",
    "\n",
    "# 2 + 3\n",
    "ax[2].scatter(sample_is[:, 1], sample_is[:, 2], alpha=0.15, s=5)\n",
    "ax[2].scatter([x_star[1]], [x_star[2]], color='red')\n",
    "ax[2].plot(x_grid, x23_vals, color='orange')\n",
    "ax[2].axhline(4.05231276e+00, color='orange', xmin=0, xmax=2.38890183e+00/4, linestyle=':', label='Polytope Boundary')\n",
    "ax[2].axvline(2.38890183e+00, color='orange', ymin=0, ymax=0.96, linestyle=':')\n",
    "\n",
    "# other plot features\n",
    "ax[0].set_xlabel(r'$x_1$')\n",
    "ax[0].set_ylabel(r'$x_2$')\n",
    "ax[1].set_xlabel(r'$x_1$')\n",
    "ax[1].set_ylabel(r'$x_3$')\n",
    "ax[2].set_xlabel(r'$x_2$')\n",
    "ax[2].set_ylabel(r'$x_3$')\n",
    "# ax[0].set_title(r'$x_1$ by $x_2$')\n",
    "# ax[1].set_title(r'$x_1$ by $x_3$')\n",
    "# ax[2].set_title(r'$x_2$ by $x_3$')\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at distribution of functional values\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(10, 3), sharex=True, sharey=True)\n",
    "\n",
    "# importance sampler\n",
    "ax[0].hist(sample_is @ h, bins=40, histtype='step')\n",
    "ax[0].axvline(np.dot(h, x_star), linestyle='--', color='gray')\n",
    "ax[0].set_title('Importance Sampler')\n",
    "\n",
    "# original sampler\n",
    "ax[1].hist(param_draws @ h, bins=40, histtype='step')\n",
    "ax[1].axvline(np.dot(h, x_star), linestyle='--', color='gray')\n",
    "ax[1].set_title('Original Sampler')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56475d",
   "metadata": {},
   "source": [
    "#### Look at the distribution of quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create llr objects\n",
    "llr = llrSolver_3d()\n",
    "max_q_rs = maxQuantileRS(\n",
    "    X_train=sample_is,\n",
    "    llr_solver=llr,\n",
    "    distr=stats.norm,\n",
    "    q=gamma,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "max_q_rs_orig = maxQuantileRS(\n",
    "    X_train=param_draws,\n",
    "    llr_solver=llr,\n",
    "    distr=stats.norm,\n",
    "    q=gamma,\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b70a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute quantiles\n",
    "maxq = max_q_rs.estimate(\n",
    "    num_samp=10000,\n",
    "    random_seeds=None\n",
    ")\n",
    "\n",
    "maxq_orig = max_q_rs_orig.estimate(\n",
    "    num_samp=10000,\n",
    "    random_seeds=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sorted data\n",
    "sort_idx = np.argsort(sample_is @ h)\n",
    "qoi_vals_samp = (sample_is @ h)[sort_idx]\n",
    "q_hat_samp = max_q_rs.max_quantiles[sort_idx]\n",
    "\n",
    "# create sorted data for the original sampler\n",
    "sort_idx_orig = np.argsort(param_draws @ h)\n",
    "qoi_vals_samp_orig = (param_draws @ h)[sort_idx_orig]\n",
    "q_hat_samp_orig = max_q_rs_orig.max_quantiles[sort_idx_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad924d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve for the LLR at each sampled functional value\n",
    "llr_vals_qoi_test = solve_llr_fixed_y(\n",
    "    qoi_vals=sample_is @ h,\n",
    "    y=data[OBS_IDX],\n",
    "    K=np.identity(3),\n",
    "    h=h,\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1cafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(12, 6), sharex=True, sharey=True)\n",
    "\n",
    "# new sampler\n",
    "ax[0].scatter(qoi_vals_samp, q_hat_samp, alpha=0.15, s=5)\n",
    "ax[0].axvline(np.dot(h, x_star), linestyle='--', color='gray')\n",
    "ax[0].plot(qoi_vals_samp, llr_vals_qoi_test[sort_idx], alpha=0.6, color='black')\n",
    "# ax[0].plot(qoi_vals_samp[T - 1:], max_q_pred_samp, color='red')\n",
    "ax[0].set_ylim(0.6, 1.3)\n",
    "ax[0].set_title('Importance-Like Sampler')\n",
    "\n",
    "# old sampler\n",
    "ax[1].scatter(qoi_vals_samp_orig, q_hat_samp_orig, alpha=0.15, s=5)\n",
    "ax[1].axvline(np.dot(h, x_star), linestyle='--', color='gray', label='True Functional Value')\n",
    "ax[1].plot(qoi_vals_samp, llr_vals_qoi_test[sort_idx], alpha=0.6, color='black', label=r'$\\lambda(\\mu, \\boldsymbol y)$')\n",
    "ax[1].set_title('Polytope Sampler')\n",
    "\n",
    "# other plot attributes\n",
    "ax[1].legend()\n",
    "ax[0].set_xlabel(r'Functional Value ($\\mu$)')\n",
    "ax[1].set_xlabel(r'Functional Value ($\\mu$)')\n",
    "ax[0].set_ylabel('Computed Quantile')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab210165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MQ direct\n",
    "mq_direct = direct_inversion(\n",
    "    qoi_vals=sample_is @ h,\n",
    "    llr_vals_qoi=llr_vals_qoi_test,\n",
    "    q_hat_vals=max_q_rs.max_quantiles,\n",
    "    local=False\n",
    ")\n",
    "\n",
    "# --- MQ Opt\n",
    "mq_opt = osb_int(\n",
    "    y=data[OBS_IDX], q=max_q_rs.max_quantiles.max(), K=np.identity(3), h=h\n",
    ")\n",
    "\n",
    "# --- MQmu param\n",
    "mq_mu_param = direct_inversion(\n",
    "    qoi_vals=sample_is @ h,\n",
    "    llr_vals_qoi=llr_vals_qoi_test,\n",
    "    q_hat_vals=max_q_rs.max_quantiles,\n",
    "    local=True\n",
    ")\n",
    "\n",
    "# --- MQmu func\n",
    "mq_mu_func = max_local_quantile_inversion(\n",
    "    qoi_vals=sample_is @ h,\n",
    "    llr_vals_qoi=llr_vals_qoi_test,\n",
    "    q_hat_vals=max_q_rs.max_quantiles,\n",
    "    method='rolling',\n",
    "    hyperparams={'T': 10, 'center': True}\n",
    ")[0]\n",
    "\n",
    "# --- OSB Interval\n",
    "osb = osb_int(\n",
    "    y=data[OBS_IDX], q=stats.chi2(1).ppf(1 - alpha), K=np.identity(3), h=h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- Intervals ---')\n",
    "print(f'MQ Direct : ({mq_direct[0]:.2f}, {mq_direct[1]:.2f})')\n",
    "print(f'MQ Opt    : ({mq_opt[0]:.2f}, {mq_opt[1]:.2f})')\n",
    "print(f'MQmu Param: ({mq_mu_param[0]:.2f}, {mq_mu_param[1]:.2f})')\n",
    "print(f'MQmu Func : ({mq_mu_func[0]:.2f}, {mq_mu_func[1]:.2f})')\n",
    "print(f'OSB       : ({osb[0]:.2f}, {osb[1]:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b030de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sorted data\n",
    "sort_idx = np.argsort(sample_is @ h)\n",
    "qoi_vals_samp = (sample_is @ h)[sort_idx]\n",
    "q_hat_samp = max_q_rs.max_quantiles[sort_idx]\n",
    "llr_vals_samp = llr_vals_qoi_test[sort_idx]\n",
    "\n",
    "# create rolling max\n",
    "T = 10\n",
    "max_q_pred_samp = pd.Series(q_hat_samp).rolling(T, center=True).max().dropna()\n",
    "llr_vals_roll = llr_vals_samp[T - 1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa459e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "sort_idx = np.argsort(sample_is @ h)\n",
    "qoi_vals_samp = (sample_is @ h)[sort_idx]\n",
    "llr_vals_samp = llr_vals_qoi_test[sort_idx]\n",
    "plt.scatter(sample_is @ h, max_q_rs.max_quantiles, alpha=0.05)\n",
    "plt.plot(qoi_vals_samp, llr_vals_samp)\n",
    "plt.plot(qoi_vals_samp[T - 1:], max_q_pred_samp, color='red')\n",
    "plt.axvline(-1, linestyle='--', color='gray')\n",
    "plt.ylim(0.6, 1.3)\n",
    "# plt.xlim(-1.1, -.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126bccd",
   "metadata": {},
   "source": [
    "# 2 - Generate Chains for the Parallel Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e139e",
   "metadata": {},
   "source": [
    "### Samples via Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ec061",
   "metadata": {},
   "source": [
    "__NOTE__: this is too slow to do locally. Look at `../parallel_scripts/3d_experiments/importance_sampler_X.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = 16000\n",
    "# AP_GAMMA = 0.75\n",
    "# AP_ORD = 0.5\n",
    "# param_draws_all = np.zeros(shape=(1000, M, 3))\n",
    "\n",
    "# for i in tqdm(range(1000)):\n",
    "\n",
    "#     # generate sample\n",
    "#     param_draws_all[i, :, :] = importance_sampler(\n",
    "#         data_i=data[i],\n",
    "#         M=M,\n",
    "#         ap_gamma=AP_GAMMA, ap_ord=AP_ORD,\n",
    "#         eta=eta, K=np.identity(3), h=h,\n",
    "#         mcmc_hp_dict=mcmc_dict\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20fd853",
   "metadata": {},
   "source": [
    "# 3 - Analyze Parallel Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6934c",
   "metadata": {},
   "source": [
    "__NOTE__: the files read in below are too large to store in the github repository. Please reach out to `mcstanle@alumni.cmu.edu` to gain file access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96035043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make key for interval types\n",
    "interval_type_key = {\n",
    "    0: \"Global Inverted\",\n",
    "    1: \"Global Optimized\",\n",
    "    2: \"Sliced Inverted\",\n",
    "    3: \"Sliced Optimized\",\n",
    "    4: \"OSB\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the computed quantiles\n",
    "with open(\n",
    "#     '../data/3d_experiments/numObs_1000_num_param_16000_num_quant_16000_alpha0.32_eta0.01_rolllingT10mixed_polytope_sampler.npz',\n",
    "#     '../data/3d_experiments/numObs_1000_num_param_16000_num_quant_16000_alpha0.32_eta0.01_rolllingT10mixed_polytope_sampler_t0.1.npz',\n",
    "#     '../data/3d_experiments/numObs_1000_num_param_16000_num_quant_10000_alpha0.32_eta0.01_rolllingT10_importance_sampler_t0.03.npz',\n",
    "    '../data/3d_experiments/numObs_1000_num_param_16000_num_quant_10000_alpha0.32_eta0.01_rolllingT10_importance_sampler_t0.03_bbcalib_center_roll.npz',\n",
    "    'rb'\n",
    ") as f:\n",
    "    exp_obj = np.load(f)\n",
    "    exp_intervals = exp_obj['intervals']\n",
    "    exp_qoi_vals = exp_obj['qoi_vals']\n",
    "    exp_llr_vals = exp_obj['llr_vals']\n",
    "    exp_q_hat_vals = exp_obj['q_hat_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef610590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations\n",
    "NUM_OBS = exp_intervals.shape[0]\n",
    "NUM_INTS = exp_intervals.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97cc3b",
   "metadata": {},
   "source": [
    "#### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27294ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_lengths = exp_intervals[:, :, 1] - exp_intervals[:, :, 0]\n",
    "\n",
    "for i in range(exp_intervals.shape[1]):\n",
    "    print(f'{interval_type_key[i]}: Estimated Length: {interval_lengths.mean(axis=0)[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "plt.bar(x=np.arange(5), height=interval_lengths.mean(axis=0))\n",
    "plt.vlines(\n",
    "    x=np.arange(5),\n",
    "    ymin=interval_lengths.mean(axis=0) - stats.norm.ppf(0.975) * (interval_lengths.std(axis=0) / np.sqrt(NUM_OBS)),\n",
    "    ymax=interval_lengths.mean(axis=0) + stats.norm.ppf(0.975) * (interval_lengths.std(axis=0) / np.sqrt(NUM_OBS)),\n",
    "    color='orange'\n",
    ")\n",
    "\n",
    "plt.xticks(ticks=np.arange(5), labels=[interval_type_key[i] for i in range(5)], rotation=15)\n",
    "plt.ylim(2.6, 3.05)\n",
    "plt.title('Estimated Expected Length')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb77a30",
   "metadata": {},
   "source": [
    "#### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17409044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute coverage\n",
    "coverage = np.zeros(shape=(NUM_OBS, NUM_INTS))\n",
    "for i in range(NUM_OBS):\n",
    "    for j in range(NUM_INTS):\n",
    "#         coverage[i, j] = int_cover(mu_true=np.dot(h, np.array([0, 0, 1])), interval=exp_intervals[i, j, :])\n",
    "        coverage[i, j] = int_cover(mu_true=np.dot(h, x_star), interval=exp_intervals[i, j, :])\n",
    "        \n",
    "for i in range(NUM_INTS):\n",
    "    print(f'{interval_type_key[i]}: Estimated Coverage: {coverage.mean(axis=0)[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_cis = np.zeros(shape=(NUM_INTS, 2))\n",
    "for i in range(NUM_INTS):\n",
    "    coverage_cis[i, :] = proportion_confint(\n",
    "        coverage.mean(axis=0)[i] * NUM_OBS, NUM_OBS, alpha=0.05, method='beta'\n",
    "    )\n",
    "\n",
    "print(coverage_cis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.grid(True, axis='y')\n",
    "plt.bar(x=np.arange(5), height=coverage.mean(axis=0))\n",
    "\n",
    "plt.vlines(x=np.arange(5), ymin=coverage_cis[:, 0], ymax=coverage_cis[:, 1], color='orange')\n",
    "\n",
    "plt.xticks(ticks=np.arange(5), labels=[interval_type_key[i] for i in range(5)], rotation=15)\n",
    "plt.axhline(0.68, linestyle='--', color='gray')\n",
    "plt.ylim(0.6, 0.74)\n",
    "plt.title('Estimated Coverage')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143c57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
